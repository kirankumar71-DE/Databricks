{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "159ca177-a5ba-44c3-a2d3-a5592a1bb972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the source directory for the CSV files\n",
    "source_dir = \"/Volumes/incremental_load/default/ordersdata/source/\"\n",
    "\n",
    "# Define the archive directory for processed CSV files\n",
    "archive_dir = \"/Volumes/incremental_load/default/ordersdata/archive/\"\n",
    "\n",
    "# Define the stage table name for storing the DataFrame\n",
    "stage_table = \"incremental_load.default.orders_stage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a78e177-4f7b-477e-bc02-949a1bb9a9c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV files from the source directory with header and infer schema options\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a4eaa1f-b021-494c-8fb0-3832636c81ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame to the specified stage table in overwrite mode\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(stage_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "181a318d-d3ff-47cc-92d8-02d4a435d228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#move the file to archive as it is processed to stage_table\n",
    "files = dbutils.fs.ls(source_dir)\n",
    "\n",
    "for file in files:\n",
    "    #fetch the source path\n",
    "    srcpath = file.path\n",
    "\n",
    "    #Constructing the archive path\n",
    "    targetpath = archive_dir + srcpath.split(\"/\")[-1]\n",
    "\n",
    "    #Move the file to archive\n",
    "    dbutils.fs.mv(srcpath, targetpath)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "orders_stage_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
